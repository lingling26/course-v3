{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.core.debugger import set_trace\n",
    "from fastai import datasets\n",
    "import pickle, gzip, math, torch, matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=Path('/Users/lingling26/.fastai/data/mnist.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    path=Path('/Users/lingling26/.fastai/data/mnist.pkl')\n",
    "    with open(path, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return map(tensor, (x_train,y_train,x_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = x_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_std = x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1304), tensor(0.3073))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean, train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, m, s):  return (x-m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.3863, -0.1956, -0.1956, -0.1956,  1.1773,  1.3044,  1.8002, -0.0939,\n",
       "         1.6858,  2.8171,  2.7154,  1.1900, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.0431,  0.0332,  0.7705,  1.5332,  1.7366,  2.7917,  2.7917,  2.7917,\n",
       "         2.7917,  2.7917,  2.4358,  1.7620,  2.7917,  2.6519,  2.0544,  0.3891,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244,  0.1985,  2.6010,  2.7917,  2.7917,  2.7917,\n",
       "         2.7917,  2.7917,  2.7917,  2.7917,  2.7917,  2.7663,  0.7578,  0.6180,\n",
       "         0.6180,  0.2875,  0.0713, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.1956,\n",
       "         2.3595,  2.7917,  2.7917,  2.7917,  2.7917,  2.7917,  2.0925,  1.8892,\n",
       "         2.7154,  2.6392, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244,  0.5925,  1.5586,  0.9358,  2.7917,\n",
       "         2.7917,  2.1815, -0.2846, -0.4244,  0.1222,  1.5332, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.2464, -0.4117,  1.5332,  2.7917,  0.7197, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,  1.3425,\n",
       "         2.7917,  1.9909, -0.3990, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.2846,  1.9909,  2.7917,  0.4654, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "         0.0205,  2.6392,  2.4358,  1.6095,  0.9485, -0.4117, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,  0.6053,  2.6264,  2.7917,\n",
       "         2.7917,  1.0883, -0.1066, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244,  0.1476,  1.9400,  2.7917,  2.7917,  1.4824, -0.0812,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.2210,\n",
       "         0.7578,  2.7790,  2.7917,  1.9527, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,  2.7409,  2.7917,  2.7409,\n",
       "         0.3891, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,  0.1603,  1.2281,\n",
       "         1.9019,  2.7917,  2.7917,  2.2070, -0.3990, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "         0.0713,  1.4570,  2.4866,  2.7917,  2.7917,  2.7917,  2.7536,  1.8892,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.1193,  1.0247,  2.3849,  2.7917,  2.7917,  2.7917,\n",
       "         2.7917,  2.1307,  0.5671, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.1320,  0.4146,  2.2832,  2.7917,\n",
       "         2.7917,  2.7917,  2.7917,  2.0925,  0.6053, -0.3990, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.1956,  1.7493,\n",
       "         2.3595,  2.7917,  2.7917,  2.7917,  2.7917,  2.0544,  0.5925, -0.3100,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "         0.2747,  1.7620,  2.4485,  2.7917,  2.7917,  2.7917,  2.7917,  2.6773,\n",
       "         1.2663, -0.2846, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244,  1.3044,  2.7917,  2.7917,  2.7917,\n",
       "         2.2705,  1.2917,  1.2536, -0.2210, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244,\n",
       "        -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244, -0.4244])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize(x_train[0], train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通常需要标准化\n",
    "x_train = normalize(x_train, train_mean, train_std)\n",
    "#对应的， valid set也需要标准化，但是注意，mean和std需要用train set的值\n",
    "x_valid = normalize(x_valid, train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0001)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到没有，标准化之后，均值接近0，标准差接近1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,m = x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=y_train.max()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,m,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练一个mlp模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(m,nh)/math.sqrt(m)\n",
    "b1 = torch.zeros(nh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.randn()按标准正态分布随机初始化tensor, 一般来说w1是个m*nh的矩阵，后面除以的那个math.sqrt(m) 是个初始化方法，叫做kaiming init或者he init；它的作用是使得第一层隐层输出的结果接近标准正态分布, 这只是一种简便的，近似算法；mean不一定是0，多运行几次这个w1, b1初始化和lin()函数，看下t.mean(), t.std(); 每次多值都不太一样，但是mean基本上是比较小的，std基本上是接近1的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0676,  0.0334,  0.0612,  ..., -0.0302,  0.0078, -0.0541],\n",
       "        [-0.0411, -0.0142,  0.0173,  ...,  0.0462, -0.0454,  0.0228],\n",
       "        [ 0.0002,  0.0801, -0.0266,  ...,  0.0689,  0.0006,  0.0136],\n",
       "        ...,\n",
       "        [-0.0500,  0.0359, -0.0188,  ..., -0.0617, -0.0249,  0.0336],\n",
       "        [-0.0318, -0.0076,  0.0488,  ...,  0.0034,  0.0089,  0.0269],\n",
       "        [-0.0057,  0.0205, -0.0240,  ...,  0.0737, -0.0306, -0.0255]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b): return x@w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = lin(x_valid, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1128), tensor(1.0171))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(),t.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用pytorch内置函数实现relu     \n",
    "clamp_min(x)内置函数当值小于参数x时，返回x，反之返回值本身"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = relu(lin(x_valid,w1, b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4602), tensor(0.6356))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.mean(),t1.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现relu之后，不是标准正态了，因此还是得调节一下\n",
    "kaiming he提出的方法还是刚刚调节w1的方法，不过这次不是除以math.sqrt(m), 而是math.sqrt(2/m)          \n",
    "乘以了一个2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(m, nh)*math.sqrt(2/m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = relu(lin(x_valid,w1,b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4800), tensor(0.7591))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.mean(), t1.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样发现std接近1了，但是mean大于0.5了，所以调整一下relu函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return x.clamp_min(0.) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = relu(lin(x_valid, w1, b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0200), tensor(0.7591))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.mean(),t2.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在就接近0,1了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = torch.randn(nh, 1) * math.sqrt(2/nh)\n",
    "b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(xb):\n",
    "    l1 = relu(lin(xb, w1, b1))\n",
    "    l2 = lin(l1, w2, b2)\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7 ms ± 930 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 _=model(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0329])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_valid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_valid).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "squeeze()函数把tensor中的大小为1的维度去掉，如果有多个大小为1的维度，都会被去掉，所以最好指定一下具体哪个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, target):\n",
    "    return (y.squeeze(-1) - target).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.shape\n",
    "model(x_valid).shape\n",
    "#model(x_valid).squeeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32.5076)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(model(x_valid), y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_grad(inp, targ): \n",
    "    # grad of loss with respect to output of previous layer\n",
    "    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mse()函数的求导，mse函数的输入shape是[10000,1]；2*（y-target）维度是1， 所以要unsqueeze(-1)变成[10000,1]; 然后还要除以input.shape[0]; 因为mse的时候求了和，所以求导之后要除, 等于求了个均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor.t(), 把tensor转制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_valid).shape[0]\n",
    "#w1.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "relu函数的导数是(inp>0).float(), 后面的out.g是chain rule, 把导数串联起来       \n",
    "inp是前一层的输出（就是本层的输入），back propagation的时候，把导数存在inp.g中。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(inp, out):\n",
    "    # grad of relu with respect to input activations\n",
    "    inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个矩阵的导数，就是这个矩阵的转置     \n",
    "但是对于linear的导数，我们不仅需要输出层对输入层的导数，还需要对w, 对b的导数，就是说对于linear层，一共需要3个导数。  \n",
    "w.g是对w求导，所以等于输入x，然后乘以out.g（这样才能链式法则，把后面的导数传递起来），out.g.unsqueeze(1)，为啥是1呢，因为这是这一层矩阵乘法消失的那个维度，即，input的列，output的行，所以反向传播的时候，要把这列展开;      \n",
    "input unsqueeze的为啥是-1呢，因为linear这一个module， 矩阵乘法，input贡献行，input的列会和output的行消失掉，input无法影响的是output的列，所以这一维需要unsqueeze补上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):\n",
    "    inp.g = out.g@w.t()\n",
    "    w.g = (inp.unsqueeze(-1)*out.g.unsqueeze(1)).sum(0)\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = lin(x_valid, w1, b1)\n",
    "relu1 = relu(l1)\n",
    "l2 = lin(relu1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0006],\n",
       "        [-0.0017],\n",
       "        [-0.0012],\n",
       "        ...,\n",
       "        [-0.0009],\n",
       "        [-0.0014],\n",
       "        [-0.0016]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_grad(l2, y_valid)\n",
    "l2.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back propagation理解上的几个要点。   \n",
    "1. 每一层的g，一定和这层原来的shape一样。   \n",
    "2. lin_grad函数里面，inp.shape是[10000,50], out.g的shape和out一样，是[10000,1], 链式法则要能乘起来，shape必须匹配，所以通过unsqueeze调整shape，记得broadcast规则么？把长度为1的维度自动扩充为可以匹配的维度，所以inp.unsqueeze(-1)变成[10000,50,1], out.g.unsqueeze(1)变成[10000,1,1],  (inp.unsqueeze(-1)*out.g.unsqueeze(1)) 之后[10000,50,1], 再对第一个维度10000求合，变成[50,1]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_grad(relu1, l2, w2, b2)\n",
    "relu1.unsqueeze(-1).shape\n",
    "w2.g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 50])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_grad(l1, relu1)\n",
    "l1.g\n",
    "w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # forward pass:\n",
    "    l1 = inp @ w1 + b1\n",
    "    l2 = relu(l1)\n",
    "    out = l2 @ w2 + b2\n",
    "    # we don't actually need the loss in backward!\n",
    "    loss = mse(out, targ)\n",
    "    \n",
    "    # backward pass:\n",
    "    mse_grad(out, targ)\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    relu_grad(l1, l2)\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把我们一轮求导的结果保存起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1g = w1.g.clone()\n",
    "w2g = w2.g.clone()\n",
    "b1g = b1.g.clone()\n",
    "b2g = b2.g.clone()\n",
    "ig  = x_valid.g.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面用pytorch的方法实现上面的功能（一轮正向+反向传播），看看和我们的结果是否一致。一致说明我们的实现没有问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt2 = x_valid.clone().requires_grad_(True)\n",
    "w12 = w1.clone().requires_grad_(True)\n",
    "w22 = w2.clone().requires_grad_(True)\n",
    "b12 = b1.clone().requires_grad_(True)\n",
    "b22 = b2.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inp, targ):\n",
    "    l1=inp@w12+b12\n",
    "    relu1=relu(l1)\n",
    "    l2=relu1@w22+b22\n",
    "    \n",
    "    return mse(l2, targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requires_grad_(true)方法是pytorch提供的方法，凡是需要求导的变量（forward函数里面），都需要设置requires_grad, 这样tensor才会生气grad存放的空间；不实现调用这个方法会报错；     \n",
    "这个fcc模型，需要求导的有5个变量，x_train，w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=forward(xt2, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 尝试自己实现一个pytorch api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse():\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp=inp\n",
    "        self.targ=targ\n",
    "        self.out=(self.inp.squeeze()-self.targ).pow(2).mean(0)\n",
    "        return self.out\n",
    "        \n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = 2*(self.inp.squeeze()-self.targ).unsqueeze(-1)/self.inp.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python类实现$__init__()$方法，类似c++构造函数;     \n",
    "实现$__call__()$方法，则可以像调用函数一样使用类的实例，即假设x是类的实例，则x()相当于x.$__call__()$       \n",
    "实现一个fc, 需要实现 mse, linear, relu 3个module， 每个module再根据具体情况，实现call(相当于是forward)和backward函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp=inp\n",
    "        self.out=self.inp.clamp_min(0.) - 0.5\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g * (self.inp>0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, w, b): self.w, self.b = w, b\n",
    "    \n",
    "    def __call__(self, inp):\n",
    "        self.inp=inp\n",
    "        self.out = inp@self.w+self.b\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g@self.w.t()\n",
    "        self.w.g = (self.inp.unsqueeze(-1) * self.out.unsqueeze(1)).sum(0)\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Linear(w1, b1), Relu(), Linear(w2, b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, target):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, target)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这个model类很好的诠释了init函数和call函数的使用场景；self.layers变量依次调用各个modules的init函数，init函数返回每个module的对象，放在layers变量中，call函数里，依次遍历layer，然后依次调用module的call函数；(call函数就是直接拿对象名加括号调用)，backward函数使用reversed方法，把layers这个list反转，然后依次调用backward方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.g, b1.g, w2.g, b2.g = [None] * 4\n",
    "model = Model(w1,b2,w2,b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "置空各个参数的导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.3 ms, sys: 2.6 ms, total: 27.9 ms\n",
      "Wall time: 14.2 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 991 ms, sys: 522 ms, total: 1.51 s\n",
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%time grad = model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的写法很繁琐，下面进一步简化写法，把Module类抽象出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*self.args)\n",
    "        return self.out\n",
    "    \n",
    "    def forward(self): raise Exception('not implemented')\n",
    "    def backward(self): self.bwd(self.out, *self.args)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有module其实就实现2个功能，一个forward，一个backward，上面的module类call函数实现初始化和forward，forward，backward本身只是个接口; call函数抽象了每个module被调用的时候的处理逻辑，就是先初始化参数，然后调用forward，后面relu, linear都继承了module类，所以也继承了call函数；然后relu和linear分别重载了forward和backward函数。由于module类中backward接口实际上是调用了bwd函数，所以后面子类里实现的也是bwd函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, inp): return inp.clamp_min(0.) - 0.5\n",
    "    def bwd(self, out, inp): inp.g = out.g * (inp>0.).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "    def forward(self, inp): return inp@self.w + self.b\n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = out.g@self.w.t()\n",
    "        self.w.g = torch.einsum(\"bi,bj->ij\", inp, out.g)\n",
    "        self.b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(Module):\n",
    "    def forward(self, inp, targ): return(inp.squeeze()-targ).pow(2).mean()\n",
    "    def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1)/targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.layers = [Linear(w1,b1), Relu(), Linear(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.g, b1.g, w2.g, b2.g = [None] * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.1 ms, sys: 2.25 ms, total: 28.4 ms\n",
      "Wall time: 14 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.3 ms, sys: 8.25 ms, total: 65.5 ms\n",
      "Wall time: 36 ms\n"
     ]
    }
   ],
   "source": [
    "%time model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
